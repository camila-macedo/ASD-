{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bcce10",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a0bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_module import parallel_evaluate\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from numpy import matrix \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score,f1_score,recall_score,precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score,f1_score,recall_score,precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,  precision_score, recall_score, classification_report, confusion_matrix\n",
    "import sklearn.metrics\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ec108",
   "metadata": {},
   "source": [
    "# READ MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00b9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_spearman= [pd.read_csv(n,sep=\",\", index_col=False,header=None) for n in glob.glob(r'spearman-hc/*csv')]\n",
    "autism_spearman= [pd.read_csv(n,sep=\",\", index_col=False,header=None) for n in glob.glob(r'spearman-asd/*csv')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad8bc8",
   "metadata": {},
   "source": [
    "# FLATTEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6ec722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n",
      "258\n",
      "242\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "list_replace_healthy=[]\n",
    "list_flatten_healthy=[]\n",
    "def flatten_matrix(x):\n",
    "    test1 = np.array(x)\n",
    "    flatten= matrix.flatten(test1)\n",
    "    return flatten[0:15000]\n",
    "\n",
    "for i in healthy_spearman:\n",
    "    list_replace_healthy.append(np.nan_to_num(i))\n",
    "  \n",
    "     \n",
    "for i in list_replace_healthy:\n",
    "    list_flatten_healthy.append(flatten_matrix(i))\n",
    "\n",
    "print(len(list_replace_healthy))\n",
    "print(len(list_flatten_healthy))\n",
    "\n",
    "\n",
    "list_replace_disease=[]\n",
    "list_flatten_disease=[]\n",
    "for i in autism_spearman:\n",
    "    list_replace_disease.append(np.nan_to_num(i))\n",
    "     \n",
    "for i in list_replace_disease:\n",
    "    list_flatten_disease.append(flatten_matrix(i))\n",
    "print(len(list_replace_disease))\n",
    "print(len(list_flatten_disease))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0bb52",
   "metadata": {},
   "source": [
    "# LABELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64e43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "yes_label = [1]*len(list_flatten_disease)\n",
    "no_label = [0]*len(list_flatten_healthy)\n",
    "\n",
    "labels = yes_label + no_label\n",
    "#\n",
    "print(len(labels))\n",
    "y=np.nan_to_num(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b075e",
   "metadata": {},
   "source": [
    "# SPLITING AND  STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd5c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 14884)\n",
      "(125, 14884)\n"
     ]
    }
   ],
   "source": [
    "myScaler = preprocessing.StandardScaler()\n",
    "X= np.concatenate([list_flatten_disease,list_flatten_healthy],axis=0)\n",
    "X = myScaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.25, shuffle=True, \n",
    "    random_state = 1234)\n",
    "X_train= np.nan_to_num(X_train)\n",
    "\n",
    "X_test= np.nan_to_num(X_test)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3057c",
   "metadata": {},
   "source": [
    "# CALLING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecefb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set performance\n",
      "  Model       AUC   AUC std  Accuracy  Accuracy std    Recall  Recall std  \\\n",
      "0    LR  0.656447  0.066367  0.655974      0.066041  0.663158    0.090197   \n",
      "\n",
      "   Precision  Precision std                                           clf_best  \n",
      "0   0.658194         0.0665  LogisticRegression(random_state=1234, solver='...  \n",
      "LR\n",
      "Test set performance\n",
      "AUC test: 0.6958247950819672\n",
      "Accuracy test: 0.696\n",
      "F1 score test: 0.6958247950819672\n",
      "Recall test: 0.6958247950819672\n",
      "Precision test: 0.6958247950819672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70        64\n",
      "           1       0.69      0.69      0.69        61\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.70      0.70      0.70       125\n",
      "weighted avg       0.70      0.70      0.70       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    models = ['LR']\n",
    "    pool = multiprocessing.Pool(processes=len(models))\n",
    "    results = pool.starmap(parallel_evaluate, [(model, X_train, y_train) for model in models])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Print the results\n",
    "    print('Train set performance')\n",
    "    print(df_results)\n",
    "    \n",
    "    # Test set performance for the best model\n",
    "    for model,best_model in zip(df_results['Model'],df_results['clf_best']):\n",
    "   # best_model = df_results.loc[df_results['AUC'].idxmax()]['clf_best']\n",
    "       # y_test1 = y_test.to_numpy()\n",
    "      #  y_test1 = y_test1.reshape(len(y_test1),)\n",
    "        n_classes = len(np.unique(y_test))\n",
    "        y_test1= label_binarize(y_test, classes=np.arange(n_classes))\n",
    "       \n",
    "        print(model)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        y_pred1 = label_binarize(y_pred_test, classes=np.arange(n_classes))\n",
    "        print('Test set performance')\n",
    "        print('AUC test:',   roc_auc_score(y_test,y_pred_test))\n",
    "        print('Accuracy test:', accuracy_score(y_test,y_pred_test))\n",
    "        print('F1 score test:', f1_score(y_test,y_pred_test, average=\"macro\", pos_label=0))\n",
    "        print('Recall test:', recall_score(y_test,y_pred_test,average=\"macro\", pos_label=0))\n",
    "        print('Precision test:', precision_score(y_test,y_pred_test,average=\"macro\", pos_label=0))\n",
    "\n",
    "        print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53693cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: LR\n",
      "Best Model Parameters:\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1234, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "best_model_idx = df_results['AUC'].idxmax()\n",
    "best_model_name = df_results.loc[best_model_idx, 'Model']\n",
    "best_model_params = df_results.loc[best_model_idx, 'clf_best'].get_params()\n",
    "\n",
    "print(f'\\nBest Model: {best_model_name}')\n",
    "print('Best Model Parameters:')\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cc938",
   "metadata": {},
   "source": [
    "# CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix')\n",
    "list_names=['TD','ASD']\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "cm_perc = cm / cm_sum.astype(float) * 100\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "nrows, ncols = cm.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        c = cm[i, j]\n",
    "        p = cm_perc[i, j]\n",
    "        if i == j:\n",
    "            s = cm_sum[i]\n",
    "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "        elif c == 0:\n",
    "            annot[i, j] = ''\n",
    "        else:\n",
    "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "cm = pd.DataFrame(cm)\n",
    "   # print(cm)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "#fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.heatmap(cm, annot=annot, fmt='',cmap='rocket_r',xticklabels=list_names, yticklabels=list_names)\n",
    "#plt.savefig(\"CM/confusion-matrix-mlp.pdf\",dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9d97",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Roc curve')\n",
    "n_classes =2\n",
    "\n",
    "t1=sum(x==0 for x in y_pred_test-y_test)/len(y_test)\n",
    "\n",
    "    ### MACRO\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(pd.get_dummies(y_test))[:, i], np.array(pd.get_dummies(y_pred_test))[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "lw=2\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='slategray', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['purple', 'lightseagreen'])\n",
    "for i, color, name in zip(range(n_classes), colors,list_names):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label=\"ROC curve of class \"+ name + \" (area = {1:0.2f})\".format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], \"k--\",color='#cb416b', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.annotate(' Random Guess',(.5,.48),color='#cb416b')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) curve\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a0c8c",
   "metadata": {},
   "source": [
    "# LEARNING CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sizes = np.linspace(0.1, 1, 10)\n",
    "visualizer = LearningCurve(\n",
    "    best_model, cv=10, scoring='accuracy', train_sizes=sizes, n_jobs=4\n",
    ")\n",
    "\n",
    "visualizer.fit(X, y)\n",
    "# Fit the data to the visualizer\n",
    "visualizer.show(outpath=\"Learning-curve-SVM-multiclas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd02f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
